(function(){"use strict";importScripts("https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"),console.log("[Worker] Starting optimized mobile version..."),ort.env.wasm.wasmPaths="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/",ort.env.wasm.simd=!1,ort.env.wasm.numThreads=1;let s=null,o=!1;self.onmessage=async l=>{const{type:r,modelUrl:a,tensorData:f,dims:c}=l.data;if(r==="loadModel")try{console.log("[Worker] Loading model with mobile-optimized settings...");const e=performance.now(),t={executionProviders:["wasm"],graphOptimizationLevel:"basic",enableCpuMemArena:!1,enableMemPattern:!1,executionMode:"sequential",enableProfiling:!1};try{s=await ort.InferenceSession.create(a,t)}catch{console.warn("[Worker] First load attempt failed, trying fallback..."),t.externalData=!1,s=await ort.InferenceSession.create(a,t)}console.log(`[Worker] Model loaded in ${(performance.now()-e).toFixed(1)} ms`),self.postMessage({type:"loaded"})}catch(e){self.postMessage({type:"error",message:`Model load failed: ${e.message}`})}if(r==="infer"){if(!s||o)return;o=!0;try{const e=performance.now(),t=new ort.Tensor("float32",new Float32Array(f),c),i={[s.inputNames[0]]:t},n=(await s.run(i))[s.outputNames[0]];self.postMessage({type:"inference",data:n.data.buffer,dims:n.dims,inferTime:(performance.now()-e).toFixed(1)},[n.data.buffer])}catch(e){self.postMessage({type:"error",message:`Inference failed: ${e.message}`})}finally{o=!1}}}})();
